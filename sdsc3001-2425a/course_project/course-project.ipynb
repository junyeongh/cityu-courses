{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDSC3001 - Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "set1 = {\"a\", \"b\", \"c\", \"d\"}\n",
    "set2 = {\"c\", \"d\", \"e\", \"f\"}\n",
    "\n",
    "similarity = jaccard_similarity(set1, set2)\n",
    "print(f\"Jaccard Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketching techniques for the Jaccard similarity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "# Parameters\n",
    "k = 128  # Number of hash functions\n",
    "n = 10_000  # cardinality of the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# print(os.getcwd() in sys.path)\n",
    "sys.path.append(os.getcwd())\n",
    "# print(os.getcwd() in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashSketch import MinHash, B_bitMinHash, OddSketch, MaxLogHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Synthetic datasets\n",
    "\n",
    "Generate set A by randomly selecting n different numbers from I\n",
    "\n",
    "Generate set B by randomly selecting $|A \\cup B| = \\frac{J_{A, B}|A|}{1+J_{A, B}}$ different numbers from set A and $n - |A \\cup B|$ different numbers from set I\\A\n",
    "\n",
    "n = 10,000 by default\n",
    "\n",
    "- Balanced set-pairs (i.e., |A| = |B| = n)\n",
    "- Unbalanced set-pairs (i.e., |A| != |B|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntheticDataset.py\n",
    "def compare_all_methods(stream, num_runs, k=128, n=10000): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "df_balance_mean = pl.read_parquet(\"result/balanced_mean_results.parquet\")\n",
    "df_balance_median = pl.read_parquet(\"result/balanced_median_results.parquet\")\n",
    "df_balance_std = pl.read_parquet(\"result/balanced_std_results.parquet\")\n",
    "df_balance_rmse = pl.read_parquet(\"result/balanced_rmse_results.parquet\")\n",
    "df_balance_bias = pl.read_parquet(\"result/balanced_bias_results.parquet\")\n",
    "\n",
    "df_unbalance_mean = pl.read_parquet(\"result/unbalance_mean_results.parquest\")\n",
    "df_unbalance_median = pl.read_parquet(\"result/unbalance_median_results.parquest\")\n",
    "df_unbalance_std = pl.read_parquet(\"result/unbalance_std_results.parquest\")\n",
    "df_unbalance_rmse = pl.read_parquet(\"result/unbalance_rmse_results.parquest\")\n",
    "df_unbalance_bias = pl.read_parquet(\"result/unbalance_bias_results.parquest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    stream = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for record_id, line in enumerate(file):\n",
    "            items = [int(x) for x in line.strip().split()]\n",
    "            for item in items:\n",
    "                stream.append([record_id, item])\n",
    "    #     dataset = pl.DataFrame(np.loadtxt(file, dtype=int))\n",
    "\n",
    "    # item_record_pairs = {}\n",
    "    # for record_id, record in enumerate(dataset):\n",
    "    #     for item in record:\n",
    "    #         if item not in item_record_pairs:\n",
    "    #             item_record_pairs[item] = []\n",
    "    #         item_record_pairs[item].append(record_id)\n",
    "    # pairs = [(item, rec) for item, recs in item_record_pairs.items() for rec in recs]\n",
    "    # print(dataset.head(5))\n",
    "    # print(f\"{dataset.shape[0]} records with {len(item_record_pairs)} distinct times\")\n",
    "    # print(f\"{len(pairs)} item-record pairs\")\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUSHROOM dataset\n",
    "\n",
    "- 8,124 records with 119 distinct items\n",
    "- 186,852 item-record pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_dataset_path = \"./data/mushroom.dat\"\n",
    "mushroom_stream = load_dataset(mushroom_dataset_path)\n",
    "\n",
    "# mushroom_dataset = mushroom[0]\n",
    "# mushroom_item_record_pairs = mushroom[1]\n",
    "# mushroom_pairs = mushroom[2]\n",
    "\n",
    "# print(mushroom_dataset[:5])\n",
    "# print(mushroom_dataset.shape)\n",
    "# print(len(mushroom_item_record_pairs))\n",
    "# print(len(mushroom_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHash(k)\n",
    "bbit_minhash = B_bitMinHash(k, b=4)\n",
    "odd_sketch = OddSketch(k, z=4 * k)\n",
    "maxlog = MaxLogHash(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinHash estimation\n",
    "minhash.process_stream(mushroom_stream)\n",
    "print(minhash.estimate_similarity(0, 1))\n",
    "\n",
    "# b-bit MinHash estimation\n",
    "bbit_minhash.process_stream(mushroom_stream)\n",
    "print(bbit_minhash.estimate_similarity(0, 1))\n",
    "\n",
    "# Odd Sketch estimation\n",
    "odd_sketch.process_stream(mushroom_stream)\n",
    "print(odd_sketch.estimate_similarity(0, 1))\n",
    "\n",
    "# MaxLogHash estimation\n",
    "maxlog.process_stream(mushroom_stream)\n",
    "print(maxlog.estimate_similarity(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONNECT dataset\n",
    "\n",
    "- 67,557 records with 127 distinct items\n",
    "- 2,904,951 item-record pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_dataset_path = \"./data/connect.dat\"\n",
    "connect_stream = load_dataset(connect_dataset_path)\n",
    "\n",
    "# connect_dataset = connect[0]\n",
    "# connect_item_record_pairs = connect[1]\n",
    "# connect_pairs = connect[2]\n",
    "\n",
    "# print(connect_dataset[:5])\n",
    "# print(connect_dataset.shape)\n",
    "# print(len(connect_item_record_pairs))\n",
    "# print(len(connect_pairs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
